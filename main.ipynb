{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Projekt ML - Przewidywanie Intencji Zakupowych Online Shoppers\n",
        "\n",
        "## Cel Projektu\n",
        "Klasyfikacja binarna przewidująca, czy użytkownik dokona zakupu (kolumna `Revenue`) na podstawie danych o zachowaniu na stronie e-commerce.\n",
        "\n",
        "## Uwaga\n",
        "Klasy OOP są zdefiniowane w tym notebooku. Alternatywnie można zaimportować klasy z pliku `ml_classes.py`:\n",
        "```python\n",
        "from ml_classes import DataLoader, DataPreprocessor, DataAnalyzer, FeatureEngineer, ModelTrainer, HyperparameterTuner\n",
        "```\n",
        "Plik `ml_classes.py` jest używany przez testy jednostkowe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import bibliotek\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Konfiguracja wizualizacji\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Definicja klas OOP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    \"\"\"Klasa odpowiedzialna za wczytanie danych z pliku CSV\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.data = None\n",
        "        self.path = None\n",
        "    \n",
        "    def load_data(self, path: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Wczytuje dane z pliku CSV\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        path : str\n",
        "            Ścieżka do pliku CSV\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            Wczytane dane\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.data = pd.read_csv(path)\n",
        "            self.path = path\n",
        "            print(f\"Dane wczytane pomyślnie z: {path}\")\n",
        "            print(f\"Kształt danych: {self.data.shape}\")\n",
        "            return self.data\n",
        "        except Exception as e:\n",
        "            print(f\"Błąd podczas wczytania danych: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def get_info(self) -> dict:\n",
        "        \"\"\"\n",
        "        Zwraca podstawowe informacje o zbiorze danych\n",
        "        \n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Słownik z informacjami o danych\n",
        "        \"\"\"\n",
        "        if self.data is None:\n",
        "            return {\"error\": \"Dane nie zostały wczytane\"}\n",
        "        \n",
        "        info = {\n",
        "            \"shape\": self.data.shape,\n",
        "            \"columns\": list(self.data.columns),\n",
        "            \"dtypes\": self.data.dtypes.to_dict(),\n",
        "            \"missing_values\": self.data.isnull().sum().to_dict(),\n",
        "            \"memory_usage\": self.data.memory_usage(deep=True).sum()\n",
        "        }\n",
        "        return info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataPreprocessor:\n",
        "    \"\"\"Klasa odpowiedzialna za preprocessing danych\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.label_encoders = {}\n",
        "        self.scaler = StandardScaler()\n",
        "        self.is_fitted = False\n",
        "    \n",
        "    def handle_missing_values(self, df: pd.DataFrame, strategy: str = 'drop') -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Obsługuje brakujące wartości\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame do przetworzenia\n",
        "        strategy : str\n",
        "            Strategia obsługi ('drop', 'mean', 'median', 'mode')\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            DataFrame po obsłudze brakujących wartości\n",
        "        \"\"\"\n",
        "        df_processed = df.copy()\n",
        "        \n",
        "        if strategy == 'drop':\n",
        "            df_processed = df_processed.dropna()\n",
        "        elif strategy == 'mean':\n",
        "            numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "            df_processed[numeric_cols] = df_processed[numeric_cols].fillna(df_processed[numeric_cols].mean())\n",
        "        elif strategy == 'median':\n",
        "            numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "            df_processed[numeric_cols] = df_processed[numeric_cols].fillna(df_processed[numeric_cols].median())\n",
        "        elif strategy == 'mode':\n",
        "            for col in df_processed.columns:\n",
        "                df_processed[col] = df_processed[col].fillna(df_processed[col].mode()[0] if not df_processed[col].mode().empty else 0)\n",
        "        \n",
        "        return df_processed\n",
        "    \n",
        "    def encode_categorical(self, df: pd.DataFrame, columns: list = None, method: str = 'label') -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Koduje zmienne kategoryczne\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame do przetworzenia\n",
        "        columns : list\n",
        "            Lista kolumn do zakodowania (None = automatyczne wykrycie)\n",
        "        method : str\n",
        "            Metoda kodowania ('label' lub 'onehot')\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            DataFrame z zakodowanymi zmiennymi\n",
        "        \"\"\"\n",
        "        df_encoded = df.copy()\n",
        "        \n",
        "        if columns is None:\n",
        "            # Automatyczne wykrycie kolumn kategorycznych\n",
        "            categorical_cols = df_encoded.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "        else:\n",
        "            categorical_cols = columns\n",
        "        \n",
        "        if method == 'label':\n",
        "            for col in categorical_cols:\n",
        "                if col in df_encoded.columns:\n",
        "                    le = LabelEncoder()\n",
        "                    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
        "                    self.label_encoders[col] = le\n",
        "        elif method == 'onehot':\n",
        "            df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, prefix=categorical_cols)\n",
        "        \n",
        "        return df_encoded\n",
        "    \n",
        "    def normalize_features(self, df: pd.DataFrame, columns: list = None, fit: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Normalizuje zmienne numeryczne\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame do przetworzenia\n",
        "        columns : list\n",
        "            Lista kolumn do normalizacji (None = wszystkie numeryczne)\n",
        "        fit : bool\n",
        "            Czy dopasować scaler (True dla train, False dla test)\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            DataFrame z znormalizowanymi zmiennymi\n",
        "        \"\"\"\n",
        "        df_normalized = df.copy()\n",
        "        \n",
        "        if columns is None:\n",
        "            numeric_cols = df_normalized.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        else:\n",
        "            numeric_cols = columns\n",
        "        \n",
        "        if fit:\n",
        "            df_normalized[numeric_cols] = self.scaler.fit_transform(df_normalized[numeric_cols])\n",
        "            self.is_fitted = True\n",
        "        else:\n",
        "            if not self.is_fitted:\n",
        "                raise ValueError(\"Scaler nie został dopasowany. Użyj fit=True dla danych treningowych.\")\n",
        "            df_normalized[numeric_cols] = self.scaler.transform(df_normalized[numeric_cols])\n",
        "        \n",
        "        return df_normalized\n",
        "    \n",
        "    def preprocess_pipeline(self, df: pd.DataFrame, target_col: str = None, \n",
        "                           handle_missing: str = 'drop', encode_method: str = 'label',\n",
        "                           normalize: bool = True, fit: bool = True) -> tuple:\n",
        "        \"\"\"\n",
        "        Pełny pipeline preprocessingu\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame do przetworzenia\n",
        "        target_col : str\n",
        "            Nazwa kolumny docelowej (jeśli None, nie separuje)\n",
        "        handle_missing : str\n",
        "            Strategia obsługi brakujących wartości\n",
        "        encode_method : str\n",
        "            Metoda kodowania kategorycznych\n",
        "        normalize : bool\n",
        "            Czy normalizować zmienne\n",
        "        fit : bool\n",
        "            Czy dopasować transformatory (True dla train)\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            (X, y) lub (df_processed, None) jeśli target_col=None\n",
        "        \"\"\"\n",
        "        df_processed = df.copy()\n",
        "        \n",
        "        # Obsługa brakujących wartości\n",
        "        df_processed = self.handle_missing_values(df_processed, strategy=handle_missing)\n",
        "        \n",
        "        # Separacja targetu jeśli podany\n",
        "        if target_col and target_col in df_processed.columns:\n",
        "            y = df_processed[target_col].copy()\n",
        "            X = df_processed.drop(columns=[target_col])\n",
        "        else:\n",
        "            X = df_processed\n",
        "            y = None\n",
        "        \n",
        "        # Kodowanie kategorycznych\n",
        "        X = self.encode_categorical(X, method=encode_method)\n",
        "        \n",
        "        # Normalizacja\n",
        "        if normalize:\n",
        "            X = self.normalize_features(X, fit=fit)\n",
        "        \n",
        "        if y is not None:\n",
        "            return X, y\n",
        "        else:\n",
        "            return X, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataAnalyzer:\n",
        "    \"\"\"Klasa odpowiedzialna za analizę danych i wizualizacje\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def descriptive_statistics(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Zwraca statystyki opisowe\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame do analizy\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            Statystyki opisowe\n",
        "        \"\"\"\n",
        "        return df.describe()\n",
        "    \n",
        "    def correlation_analysis(self, df: pd.DataFrame, target: str = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Analiza korelacji\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame do analizy\n",
        "        target : str\n",
        "            Nazwa kolumny docelowej\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            Macierz korelacji\n",
        "        \"\"\"\n",
        "        if target and target in df.columns:\n",
        "            correlations = df.corr()[target].sort_values(ascending=False)\n",
        "            return correlations\n",
        "        else:\n",
        "            return df.corr()\n",
        "    \n",
        "    def visualize_distributions(self, df: pd.DataFrame, columns: list = None, figsize: tuple = (15, 10)):\n",
        "        \"\"\"\n",
        "        Wizualizuje rozkłady zmiennych numerycznych\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame do wizualizacji\n",
        "        columns : list\n",
        "            Lista kolumn do wizualizacji (None = wszystkie numeryczne)\n",
        "        figsize : tuple\n",
        "            Rozmiar wykresu\n",
        "        \"\"\"\n",
        "        if columns is None:\n",
        "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        else:\n",
        "            numeric_cols = columns\n",
        "        \n",
        "        n_cols = len(numeric_cols)\n",
        "        n_rows = (n_cols + 2) // 3\n",
        "        \n",
        "        fig, axes = plt.subplots(n_rows, 3, figsize=figsize)\n",
        "        axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "        \n",
        "        for idx, col in enumerate(numeric_cols):\n",
        "            if idx < len(axes):\n",
        "                df[col].hist(bins=30, ax=axes[idx], edgecolor='black')\n",
        "                axes[idx].set_title(f'Rozkład {col}')\n",
        "                axes[idx].set_xlabel(col)\n",
        "                axes[idx].set_ylabel('Częstość')\n",
        "        \n",
        "        # Ukryj puste subploty\n",
        "        for idx in range(len(numeric_cols), len(axes)):\n",
        "            axes[idx].set_visible(False)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def visualize_correlations(self, df: pd.DataFrame, figsize: tuple = (12, 10)):\n",
        "        \"\"\"\n",
        "        Wizualizuje macierz korelacji\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame do wizualizacji\n",
        "        figsize : tuple\n",
        "            Rozmiar wykresu\n",
        "        \"\"\"\n",
        "        numeric_df = df.select_dtypes(include=[np.number])\n",
        "        corr_matrix = numeric_df.corr()\n",
        "        \n",
        "        plt.figure(figsize=figsize)\n",
        "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "                   center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "        plt.title('Macierz korelacji')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def class_balance_analysis(self, y: pd.Series) -> dict:\n",
        "        \"\"\"\n",
        "        Analiza balansu klas\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        y : pd.Series\n",
        "            Zmienna docelowa\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Słownik z informacjami o balansie klas\n",
        "        \"\"\"\n",
        "        value_counts = y.value_counts()\n",
        "        percentages = y.value_counts(normalize=True) * 100\n",
        "        \n",
        "        analysis = {\n",
        "            \"counts\": value_counts.to_dict(),\n",
        "            \"percentages\": percentages.to_dict(),\n",
        "            \"is_balanced\": (percentages.min() > 40) and (percentages.max() < 60)\n",
        "        }\n",
        "        \n",
        "        # Wizualizacja\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        value_counts.plot(kind='bar', color=['skyblue', 'salmon'])\n",
        "        plt.title('Rozkład klas docelowych')\n",
        "        plt.xlabel('Klasa')\n",
        "        plt.ylabel('Liczba obserwacji')\n",
        "        plt.xticks(rotation=0)\n",
        "        for i, v in enumerate(value_counts.values):\n",
        "            plt.text(i, v, str(v), ha='center', va='bottom')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureEngineer:\n",
        "    \"\"\"Klasa odpowiedzialna za feature engineering\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.feature_importance = None\n",
        "    \n",
        "    def create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Tworzy cechy interakcyjne\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame do przetworzenia\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            DataFrame z nowymi cechami\n",
        "        \"\"\"\n",
        "        df_new = df.copy()\n",
        "        \n",
        "        # TotalPages = suma wszystkich stron\n",
        "        if all(col in df_new.columns for col in ['Administrative', 'Informational', 'ProductRelated']):\n",
        "            df_new['TotalPages'] = (df_new['Administrative'] + \n",
        "                                   df_new['Informational'] + \n",
        "                                   df_new['ProductRelated'])\n",
        "        \n",
        "        # TotalDuration = suma wszystkich czasów\n",
        "        if all(col in df_new.columns for col in ['Administrative_Duration', \n",
        "                                                  'Informational_Duration', \n",
        "                                                  'ProductRelated_Duration']):\n",
        "            df_new['TotalDuration'] = (df_new['Administrative_Duration'] + \n",
        "                                       df_new['Informational_Duration'] + \n",
        "                                       df_new['ProductRelated_Duration'])\n",
        "        \n",
        "        # AvgPageDuration = średni czas na stronę\n",
        "        if 'TotalDuration' in df_new.columns and 'TotalPages' in df_new.columns:\n",
        "            df_new['AvgPageDuration'] = df_new['TotalDuration'] / (df_new['TotalPages'] + 1e-6)\n",
        "        \n",
        "        # BounceExitRatio = stosunek bounce do exit\n",
        "        if all(col in df_new.columns for col in ['BounceRates', 'ExitRates']):\n",
        "            df_new['BounceExitRatio'] = df_new['BounceRates'] / (df_new['ExitRates'] + 1e-6)\n",
        "        \n",
        "        # ProductRelatedRatio = stosunek stron produktowych do wszystkich\n",
        "        if 'ProductRelated' in df_new.columns and 'TotalPages' in df_new.columns:\n",
        "            df_new['ProductRelatedRatio'] = df_new['ProductRelated'] / (df_new['TotalPages'] + 1e-6)\n",
        "        \n",
        "        return df_new\n",
        "    \n",
        "    def create_aggregated_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Tworzy cechy zagregowane\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame do przetworzenia\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            DataFrame z nowymi cechami\n",
        "        \"\"\"\n",
        "        df_new = df.copy()\n",
        "        \n",
        "        # Można dodać więcej cech zagregowanych tutaj\n",
        "        # Na przykład: średnie, mediany, itp.\n",
        "        \n",
        "        return df_new\n",
        "    \n",
        "    def select_features(self, X: pd.DataFrame, y: pd.Series, \n",
        "                       method: str = 'correlation', threshold: float = 0.01) -> tuple:\n",
        "        \"\"\"\n",
        "        Selekcja zmiennych\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pd.DataFrame\n",
        "            Cechy\n",
        "        y : pd.Series\n",
        "            Zmienna docelowa\n",
        "        method : str\n",
        "            Metoda selekcji ('correlation', 'importance')\n",
        "        threshold : float\n",
        "            Próg dla selekcji\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            (X_selected, selected_features)\n",
        "        \"\"\"\n",
        "        if method == 'correlation':\n",
        "            correlations = X.corrwith(y).abs()\n",
        "            selected_features = correlations[correlations >= threshold].index.tolist()\n",
        "            X_selected = X[selected_features]\n",
        "        \n",
        "        elif method == 'importance':\n",
        "            # Użyj Random Forest do określenia ważności cech\n",
        "            rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "            rf.fit(X, y)\n",
        "            \n",
        "            feature_importance = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "            self.feature_importance = feature_importance.sort_values(ascending=False)\n",
        "            \n",
        "            selected_features = feature_importance[feature_importance >= threshold].index.tolist()\n",
        "            X_selected = X[selected_features]\n",
        "        \n",
        "        else:\n",
        "            X_selected = X\n",
        "            selected_features = X.columns.tolist()\n",
        "        \n",
        "        return X_selected, selected_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ModelTrainer:\n",
        "    \"\"\"Klasa odpowiedzialna za trenowanie modeli\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "    \n",
        "    def train_model(self, X_train, y_train, model_type: str, **kwargs):\n",
        "        \"\"\"\n",
        "        Trenuje model\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_train : array-like\n",
        "            Cechy treningowe\n",
        "        y_train : array-like\n",
        "            Zmienna docelowa treningowa\n",
        "        model_type : str\n",
        "            Typ modelu ('logistic', 'random_forest', 'svm', 'xgboost')\n",
        "        **kwargs\n",
        "            Dodatkowe parametry modelu\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        model\n",
        "            Wytrenowany model\n",
        "        \"\"\"\n",
        "        if model_type == 'logistic':\n",
        "            model = LogisticRegression(random_state=42, max_iter=1000, **kwargs)\n",
        "        elif model_type == 'random_forest':\n",
        "            model = RandomForestClassifier(random_state=42, n_jobs=-1, **kwargs)\n",
        "        elif model_type == 'svm':\n",
        "            model = SVC(random_state=42, probability=True, **kwargs)\n",
        "        elif model_type == 'xgboost':\n",
        "            try:\n",
        "                import xgboost as xgb\n",
        "                model = xgb.XGBClassifier(random_state=42, **kwargs)\n",
        "            except ImportError:\n",
        "                print(\"XGBoost nie jest zainstalowany. Używam Random Forest zamiast tego.\")\n",
        "                model = RandomForestClassifier(random_state=42, n_jobs=-1, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(f\"Nieznany typ modelu: {model_type}\")\n",
        "        \n",
        "        model.fit(X_train, y_train)\n",
        "        self.models[model_type] = model\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    def evaluate_model(self, model, X_test, y_test) -> dict:\n",
        "        \"\"\"\n",
        "        Ewaluuje model\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        model : model\n",
        "            Model do ewaluacji\n",
        "        X_test : array-like\n",
        "            Cechy testowe\n",
        "        y_test : array-like\n",
        "            Zmienna docelowa testowa\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Słownik z metrykami\n",
        "        \"\"\"\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "        \n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y_test, y_pred),\n",
        "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "            'f1_score': f1_score(y_test, y_pred, zero_division=0),\n",
        "            'roc_auc': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None,\n",
        "            'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
        "        }\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    def compare_models(self, models: dict, X_test, y_test) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Porównuje wiele modeli\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        models : dict\n",
        "            Słownik modeli {nazwa: model}\n",
        "        X_test : array-like\n",
        "            Cechy testowe\n",
        "        y_test : array-like\n",
        "            Zmienna docelowa testowa\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            DataFrame z wynikami porównania\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        \n",
        "        for name, model in models.items():\n",
        "            metrics = self.evaluate_model(model, X_test, y_test)\n",
        "            metrics['model'] = name\n",
        "            results.append(metrics)\n",
        "        \n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_df = results_df.set_index('model')\n",
        "        \n",
        "        return results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HyperparameterTuner:\n",
        "    \"\"\"Klasa odpowiedzialna za optymalizację hiperparametrów\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.best_params = {}\n",
        "        self.best_scores = {}\n",
        "    \n",
        "    def grid_search(self, model, param_grid: dict, X_train, y_train, \n",
        "                   cv: int = 5, scoring: str = 'f1', n_jobs: int = -1):\n",
        "        \"\"\"\n",
        "        Grid Search dla optymalizacji hiperparametrów\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        model : model\n",
        "            Model do optymalizacji\n",
        "        param_grid : dict\n",
        "            Siatka parametrów\n",
        "        X_train : array-like\n",
        "            Cechy treningowe\n",
        "        y_train : array-like\n",
        "            Zmienna docelowa treningowa\n",
        "        cv : int\n",
        "            Liczba foldów cross-validation\n",
        "        scoring : str\n",
        "            Metryka do optymalizacji\n",
        "        n_jobs : int\n",
        "            Liczba równoległych zadań\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        model\n",
        "            Najlepszy model\n",
        "        \"\"\"\n",
        "        grid_search = GridSearchCV(\n",
        "            model, \n",
        "            param_grid, \n",
        "            cv=cv, \n",
        "            scoring=scoring, \n",
        "            n_jobs=n_jobs,\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        grid_search.fit(X_train, y_train)\n",
        "        \n",
        "        self.best_params[type(model).__name__] = grid_search.best_params_\n",
        "        self.best_scores[type(model).__name__] = grid_search.best_score_\n",
        "        \n",
        "        print(f\"Najlepsze parametry: {grid_search.best_params_}\")\n",
        "        print(f\"Najlepszy wynik CV: {grid_search.best_score_:.4f}\")\n",
        "        \n",
        "        return grid_search.best_estimator_\n",
        "    \n",
        "    def random_search(self, model, param_distributions: dict, X_train, y_train,\n",
        "                     n_iter: int = 50, cv: int = 5, scoring: str = 'f1', n_jobs: int = -1):\n",
        "        \"\"\"\n",
        "        Random Search dla optymalizacji hiperparametrów\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        model : model\n",
        "            Model do optymalizacji\n",
        "        param_distributions : dict\n",
        "            Rozkłady parametrów\n",
        "        X_train : array-like\n",
        "            Cechy treningowe\n",
        "        y_train : array-like\n",
        "            Zmienna docelowa treningowa\n",
        "        n_iter : int\n",
        "            Liczba iteracji\n",
        "        cv : int\n",
        "            Liczba foldów cross-validation\n",
        "        scoring : str\n",
        "            Metryka do optymalizacji\n",
        "        n_jobs : int\n",
        "            Liczba równoległych zadań\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        model\n",
        "            Najlepszy model\n",
        "        \"\"\"\n",
        "        random_search = RandomizedSearchCV(\n",
        "            model,\n",
        "            param_distributions,\n",
        "            n_iter=n_iter,\n",
        "            cv=cv,\n",
        "            scoring=scoring,\n",
        "            n_jobs=n_jobs,\n",
        "            random_state=42,\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        random_search.fit(X_train, y_train)\n",
        "        \n",
        "        self.best_params[type(model).__name__] = random_search.best_params_\n",
        "        self.best_scores[type(model).__name__] = random_search.best_score_\n",
        "        \n",
        "        print(f\"Najlepsze parametry: {random_search.best_params_}\")\n",
        "        print(f\"Najlepszy wynik CV: {random_search.best_score_:.4f}\")\n",
        "        \n",
        "        return random_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Etap 1: Pobranie Danych\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicjalizacja DataLoader\n",
        "loader = DataLoader()\n",
        "\n",
        "# Wczytanie danych\n",
        "data = loader.load_data('online_shoppers_intention.csv')\n",
        "\n",
        "# Podstawowe informacje o zbiorze\n",
        "info = loader.get_info()\n",
        "print(\"\\n=== Podstawowe informacje o zbiorze ===\")\n",
        "print(f\"Kształt: {info['shape']}\")\n",
        "print(f\"\\nKolumny ({len(info['columns'])}):\")\n",
        "for col in info['columns']:\n",
        "    print(f\"  - {col}\")\n",
        "\n",
        "print(f\"\\nBrakujące wartości:\")\n",
        "missing = {k: v for k, v in info['missing_values'].items() if v > 0}\n",
        "if missing:\n",
        "    for col, count in missing.items():\n",
        "        print(f\"  - {col}: {count}\")\n",
        "else:\n",
        "    print(\"  Brak brakujących wartości!\")\n",
        "\n",
        "# Podgląd danych\n",
        "print(\"\\n=== Pierwsze 5 wierszy ===\")\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicjalizacja preprocessora\n",
        "preprocessor = DataPreprocessor()\n",
        "\n",
        "# Pełny pipeline preprocessingu\n",
        "X, y = preprocessor.preprocess_pipeline(\n",
        "    data, \n",
        "    target_col='Revenue',\n",
        "    handle_missing='drop',\n",
        "    encode_method='label',\n",
        "    normalize=True,\n",
        "    fit=True\n",
        ")\n",
        "\n",
        "print(f\"Kształt X: {X.shape}\")\n",
        "print(f\"Kształt y: {y.shape}\")\n",
        "print(f\"\\nRozkład klas docelowych:\")\n",
        "print(y.value_counts())\n",
        "print(f\"\\nProcentowy rozkład:\")\n",
        "print(y.value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicjalizacja analizatora\n",
        "analyzer = DataAnalyzer()\n",
        "\n",
        "# Statystyki opisowe\n",
        "print(\"=== Statystyki opisowe ===\")\n",
        "stats = analyzer.descriptive_statistics(data.select_dtypes(include=[np.number]))\n",
        "print(stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analiza korelacji z targetem\n",
        "print(\"=== Korelacje ze zmienną docelową (Revenue) ===\")\n",
        "correlations = analyzer.correlation_analysis(data, target='Revenue')\n",
        "print(correlations.sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wizualizacja macierzy korelacji\n",
        "analyzer.visualize_correlations(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analiza balansu klas\n",
        "balance_info = analyzer.class_balance_analysis(y)\n",
        "print(f\"\\nBalans klas: {'Zbalansowany' if balance_info['is_balanced'] else 'Niezbalansowany'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wizualizacja rozkładów wybranych zmiennych\n",
        "numeric_cols = ['Administrative', 'Administrative_Duration', 'ProductRelated', \n",
        "                'ProductRelated_Duration', 'BounceRates', 'ExitRates', 'PageValues']\n",
        "analyzer.visualize_distributions(data[numeric_cols + ['Revenue']], columns=numeric_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Etap 4: Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Przygotowanie danych przed feature engineering (bez normalizacji)\n",
        "X_raw, y_raw = preprocessor.preprocess_pipeline(\n",
        "    data,\n",
        "    target_col='Revenue',\n",
        "    handle_missing='drop',\n",
        "    encode_method='label',\n",
        "    normalize=False,  # Nie normalizujemy jeszcze\n",
        "    fit=True\n",
        ")\n",
        "\n",
        "# Inicjalizacja feature engineer\n",
        "feature_engineer = FeatureEngineer()\n",
        "\n",
        "# Tworzenie nowych cech\n",
        "X_with_features = feature_engineer.create_interaction_features(X_raw)\n",
        "X_with_features = feature_engineer.create_aggregated_features(X_with_features)\n",
        "\n",
        "print(f\"Liczba cech przed feature engineering: {X_raw.shape[1]}\")\n",
        "print(f\"Liczba cech po feature engineering: {X_with_features.shape[1]}\")\n",
        "print(f\"\\nNowe cechy:\")\n",
        "new_features = set(X_with_features.columns) - set(X_raw.columns)\n",
        "for feat in new_features:\n",
        "    print(f\"  - {feat}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selekcja zmiennych na podstawie ważności (Random Forest)\n",
        "X_selected, selected_features = feature_engineer.select_features(\n",
        "    X_with_features, \n",
        "    y_raw, \n",
        "    method='importance', \n",
        "    threshold=0.01\n",
        ")\n",
        "\n",
        "print(f\"Liczba wybranych cech: {len(selected_features)}\")\n",
        "print(f\"\\nWybrane cechy:\")\n",
        "for feat in selected_features:\n",
        "    print(f\"  - {feat}\")\n",
        "\n",
        "# Wizualizacja ważności cech\n",
        "if feature_engineer.feature_importance is not None:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = feature_engineer.feature_importance.head(20)\n",
        "    top_features.plot(kind='barh', color='steelblue')\n",
        "    plt.title('Top 20 najważniejszych cech (Feature Importance)')\n",
        "    plt.xlabel('Ważność')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalizacja wybranych cech\n",
        "X_final = preprocessor.normalize_features(X_selected, fit=True)\n",
        "\n",
        "print(f\"Końcowy kształt danych: {X_final.shape}\")\n",
        "print(f\"Liczba obserwacji: {X_final.shape[0]}\")\n",
        "print(f\"Liczba cech: {X_final.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Etap 5: Przygotowanie Zbiorów\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Podział na zbiór treningowy i testowy (80/20) z stratyfikacją\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_final, \n",
        "    y_raw, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y_raw\n",
        ")\n",
        "\n",
        "print(f\"Zbiór treningowy: {X_train.shape[0]} obserwacji\")\n",
        "print(f\"Zbiór testowy: {X_test.shape[0]} obserwacji\")\n",
        "print(f\"\\nRozkład klas w zbiorze treningowym:\")\n",
        "print(y_train.value_counts(normalize=True) * 100)\n",
        "print(f\"\\nRozkład klas w zbiorze testowym:\")\n",
        "print(y_test.value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Etap 6: Trenowanie Modeli\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicjalizacja trenera modeli\n",
        "trainer = ModelTrainer()\n",
        "\n",
        "# Trenowanie różnych modeli\n",
        "models = {}\n",
        "\n",
        "print(\"=== Trenowanie Logistic Regression ===\")\n",
        "models['Logistic Regression'] = trainer.train_model(\n",
        "    X_train, y_train, \n",
        "    model_type='logistic'\n",
        ")\n",
        "\n",
        "print(\"\\n=== Trenowanie Random Forest ===\")\n",
        "models['Random Forest'] = trainer.train_model(\n",
        "    X_train, y_train,\n",
        "    model_type='random_forest',\n",
        "    n_estimators=100\n",
        ")\n",
        "\n",
        "print(\"\\n=== Trenowanie SVM ===\")\n",
        "models['SVM'] = trainer.train_model(\n",
        "    X_train, y_train,\n",
        "    model_type='svm',\n",
        "    kernel='rbf',\n",
        "    C=1.0\n",
        ")\n",
        "\n",
        "print(\"\\n=== Trenowanie XGBoost (lub Random Forest jeśli XGBoost niedostępny) ===\")\n",
        "models['XGBoost'] = trainer.train_model(\n",
        "    X_train, y_train,\n",
        "    model_type='xgboost',\n",
        "    n_estimators=100\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Porównanie modeli\n",
        "print(\"=== Porównanie modeli ===\")\n",
        "comparison = trainer.compare_models(models, X_test, y_test)\n",
        "print(comparison)\n",
        "\n",
        "# Wizualizacja porównania\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Wykres słupkowy metryk\n",
        "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score']\n",
        "comparison[metrics_to_plot].plot(kind='bar', ax=axes[0], color=['skyblue', 'salmon', 'lightgreen', 'orange'])\n",
        "axes[0].set_title('Porównanie metryk modeli')\n",
        "axes[0].set_xlabel('Model')\n",
        "axes[0].set_ylabel('Wartość')\n",
        "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Tabela z wynikami\n",
        "axes[1].axis('tight')\n",
        "axes[1].axis('off')\n",
        "table = axes[1].table(cellText=comparison.round(4).values,\n",
        "                     rowLabels=comparison.index,\n",
        "                     colLabels=comparison.columns,\n",
        "                     cellLoc='center',\n",
        "                     loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(9)\n",
        "axes[1].set_title('Tabela wyników', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Etap 7: Fine-tuning (Optymalizacja Hiperparametrów)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wybór najlepszego modelu do tuningu (na podstawie wyników)\n",
        "best_model_name = comparison['f1_score'].idxmax()\n",
        "print(f\"Najlepszy model przed tuningiem: {best_model_name}\")\n",
        "print(f\"F1-score: {comparison.loc[best_model_name, 'f1_score']:.4f}\")\n",
        "\n",
        "# Inicjalizacja tunera\n",
        "tuner = HyperparameterTuner()\n",
        "\n",
        "# Optymalizacja dla Random Forest (zwykle najlepszy dla tego typu danych)\n",
        "if best_model_name == 'Random Forest' or 'Random Forest' in models:\n",
        "    print(\"\\n=== Optymalizacja Random Forest ===\")\n",
        "    rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "    \n",
        "    param_grid_rf = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, 30, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "    \n",
        "    best_rf = tuner.grid_search(\n",
        "        rf_base,\n",
        "        param_grid_rf,\n",
        "        X_train, y_train,\n",
        "        cv=5,\n",
        "        scoring='f1'\n",
        "    )\n",
        "    \n",
        "    models['Random Forest (Tuned)'] = best_rf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Porównanie przed i po tuningu\n",
        "print(\"=== Porównanie przed i po tuningu ===\")\n",
        "rf_original_metrics = trainer.evaluate_model(models['Random Forest'], X_test, y_test)\n",
        "rf_tuned_metrics = trainer.evaluate_model(models['Random Forest (Tuned)'], X_test, y_test)\n",
        "\n",
        "comparison_tuning = pd.DataFrame({\n",
        "    'Przed tuningiem': rf_original_metrics,\n",
        "    'Po tuningu': rf_tuned_metrics\n",
        "}).T\n",
        "\n",
        "print(comparison_tuning[['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']])\n",
        "\n",
        "# Wizualizacja\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "ax.bar(x - width/2, comparison_tuning.loc['Przed tuningiem', metrics], width, \n",
        "       label='Przed tuningiem', color='skyblue')\n",
        "ax.bar(x + width/2, comparison_tuning.loc['Po tuningu', metrics], width, \n",
        "       label='Po tuningu', color='salmon')\n",
        "\n",
        "ax.set_xlabel('Metryki')\n",
        "ax.set_ylabel('Wartość')\n",
        "ax.set_title('Porównanie modelu przed i po optymalizacji hiperparametrów')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Etap 8: Ewaluacja - Szczegółowa Analiza Najlepszego Modelu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wybór najlepszego modelu (po tuningu)\n",
        "final_model = models['Random Forest (Tuned)']\n",
        "final_metrics = trainer.evaluate_model(final_model, X_test, y_test)\n",
        "\n",
        "print(\"=== Szczegółowe metryki najlepszego modelu ===\")\n",
        "print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n",
        "print(f\"Precision: {final_metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {final_metrics['recall']:.4f}\")\n",
        "print(f\"F1-score: {final_metrics['f1_score']:.4f}\")\n",
        "print(f\"ROC-AUC: {final_metrics['roc_auc']:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = final_metrics['confusion_matrix']\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Wizualizacja Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['No Purchase', 'Purchase'],\n",
        "            yticklabels=['No Purchase', 'Purchase'])\n",
        "plt.title('Confusion Matrix - Random Forest (Tuned)')\n",
        "plt.ylabel('Rzeczywistość')\n",
        "plt.xlabel('Przewidywanie')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred_final, \n",
        "                          target_names=['No Purchase', 'Purchase']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curve\n",
        "y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
        "         label=f'ROC curve (AUC = {final_metrics[\"roc_auc\"]:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Random Forest (Tuned)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance dla najlepszego modelu\n",
        "if hasattr(final_model, 'feature_importances_'):\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'feature': X_final.columns,\n",
        "        'importance': final_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_20 = feature_importance_df.head(20)\n",
        "    plt.barh(range(len(top_20)), top_20['importance'], color='steelblue')\n",
        "    plt.yticks(range(len(top_20)), top_20['feature'])\n",
        "    plt.xlabel('Ważność cechy')\n",
        "    plt.title('Top 20 najważniejszych cech - Random Forest (Tuned)')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n=== Top 10 najważniejszych cech ===\")\n",
        "    print(feature_importance_df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation dla ostatecznej weryfikacji\n",
        "print(\"=== Cross-Validation (5-fold) ===\")\n",
        "cv_scores = cross_val_score(final_model, X_train, y_train, cv=5, scoring='f1')\n",
        "print(f\"F1-scores dla każdego folda: {cv_scores}\")\n",
        "print(f\"Średni F1-score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "# Wizualizacja CV scores\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(range(1, 6), cv_scores, color='lightblue', edgecolor='black')\n",
        "plt.axhline(y=cv_scores.mean(), color='red', linestyle='--', \n",
        "           label=f'Średnia: {cv_scores.mean():.4f}')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('F1-score')\n",
        "plt.title('Cross-Validation Scores (5-fold)')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Podsumowanie i Wnioski\n",
        "\n",
        "### Wyniki:\n",
        "- **Najlepszy model**: Random Forest z optymalizacją hiperparametrów\n",
        "- **Accuracy**: {final_metrics['accuracy']:.4f}\n",
        "- **F1-score**: {final_metrics['f1_score']:.4f}\n",
        "- **ROC-AUC**: {final_metrics['roc_auc']:.4f}\n",
        "\n",
        "### Analiza błędów:\n",
        "- **False Positives**: {cm[0][1]} - Model przewiduje zakup, ale użytkownik nie kupił\n",
        "- **False Negatives**: {cm[1][0]} - Model nie przewiduje zakupu, ale użytkownik kupił\n",
        "\n",
        "### Pomysły na dalszy rozwój:\n",
        "1. Przetestowanie innych algorytmów (Gradient Boosting, Neural Networks)\n",
        "2. Zbalansowanie zbioru danych (SMOTE, undersampling)\n",
        "3. Głębsza analiza feature engineering\n",
        "4. Ensemble methods (łączenie wielu modeli)\n",
        "5. Analiza błędnych predykcji w celu zrozumienia wzorców\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
